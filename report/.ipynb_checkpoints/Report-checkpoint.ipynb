{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical foundations of machine learning: project\n",
    "#### Samuel Buchet (000447808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report presents machine learning techniques used for the prediction of house prices. Multiple approches are proposed like feature selection, model selection and ensemble of models. These techniques are dicussed and combined to build a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, supervised learning techniques are used to predict prices of houses. It takes place into a Kaagle competition called \"House Prices: Advanced Regression Techniques\" (https://www.kaggle.com/). The training dataset available on the website contains a description of different features of the houses, like the year it was built and the physical location.\n",
    "\n",
    "In the first section of this report, the training set is preprocessed and a feature selection technique is used to reduce the number of features. In the second section, a model selection procedure is proposed to compare different models and select the most relevants. In the third section, a procedure which combines multiple models is proposed. The last section presents the results and proposes alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, a feature selection method is used to select relevant features. The dataset is loaded and preprocessed first. After that, the feature selection procedure is used to remove features of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the dataset needs to be preprocessed in order to be exploitable. For example, many variables of the dataset are factor variables and it also contains \"NA\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "training_dataset <- read.csv(\"../data/train.csv\")\n",
    "factor_variables <- which(sapply(training_dataset[1,],class)==\"factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe that the feature called \"MSSubClass\" is considered as an integer variable. However, according to the file \"data_description.csv\", this variable is a categorical one. So it should be modified. Moreover, the id variable is obviously not interesting for the price, so it can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turns the \"MSSubClass\" variable into a categorical variable\n",
    "training_dataset[,2] <- factor(training_dataset[,2])\n",
    "# remove the \"id\" variable\n",
    "training_dataset <- training_dataset[,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the categorical variables needs to be removed. Nevertheless, some of them are usefull to predict the prices. In order to take advantage of these variables, the package \"dummies\" can be used. Thanks to this package, these variables are transformed into integer variables with the one shot method (one variable is created for each different value of each categorical variable). \n",
    "\n",
    "However, it is not possible to proceed like this for all categorical variables. By doing this, it would create too many variables. A subset of categorical variables have been selected according to the description and their apparent usefulness in the prices. The selected variables are: \"MSSubClass\", \"HouseStyle\", \"ExterQual\", \"HeatingQC\", \"KitchenQual\" and \"Functional\"\n",
    "\n",
    "It is also necessary to remove the NA values. To do that, the values are replaced by the mean of their column, as done during the exercise sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the categorical variables\n",
    "factor_variables <- which(sapply(training_dataset[1, ], class) == \"factor\")\n",
    "training_dataset_preprocessed <- training_dataset[, -factor_variables]\n",
    "\n",
    "# remove the na values\n",
    "for (i in 1:(ncol(training_dataset_preprocessed)-1) ) {\n",
    "    val <- mean(training_dataset_preprocessed[,i], na.rm=T)\n",
    "    training_dataset_preprocessed[is.na(training_dataset_preprocessed[,i]),i] <- val\n",
    "}\n",
    "\n",
    "# select some categorical variables and turns them into integer variables\n",
    "library('dummies')\n",
    "factor_sel <- c(\"MSSubClass\", \"ExterQual\", \"HeatingQC\", \"KitchenQual\")\n",
    "data_factor_onehot <- dummy.data.frame(training_dataset[,factor_sel], sep=\"_\")\n",
    "dataset <- cbind(data_factor_onehot, training_dataset_preprocessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection technique used in this project is wrapper method. This supervised technique consists in building a set a features by selecting one feature at a time. To select a new feature, the algorithm computes the cross validation error (detailed in the next section) over the training set with all the previously selected features plus each of the remaining features. The one that gives the best result is then added to the set.\n",
    "After the selection of each feature, the selected feature is printed with the cross validation error. At the end, it is possible to select the best set of features thanks to the ouput.\n",
    "\n",
    "pseudocode:\n",
    "\n",
    "```\n",
    "selected <- empty_list\n",
    "candidates <- set_of_all_features\n",
    "\n",
    "while candidates_is_not_empty do\n",
    "   \n",
    "    errors <- list_of_size_nbCandidates\n",
    "    \n",
    "    for_each candidate do\n",
    "        errors[candidate] <- cross_validation_over(selected+candidate)\n",
    "    end_for\n",
    "    \n",
    "    print min(errors)\n",
    "    print sd(errors)\n",
    "    best_candidate <- argmin(errors)\n",
    "    selected <- selected + best_candidate\n",
    "    candidates <- candidates - best_candidate\n",
    "    \n",
    "end_while\n",
    "```\n",
    "\n",
    "The problem of this method is that it needs a model. The model selection procedure needs to be done at the same time. Moreover, this procedure can be very expensive if the number of features is big and if the learning algorithm is expensive. Here, we use a linear model and make the assumption that a feature that reduces the cost for a model after being added is also relevant for other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"feature:  58 mean error:  0.4069 sd error:  0.029\"\n",
      "[1] \"feature:  31 mean error:  0.2623 sd error:  0.0157\"\n",
      "[1] \"feature:  43 mean error:  0.2285 sd error:  0.0262\"\n",
      "[1] \"feature:  40 mean error:  0.2132 sd error:  0.0224\"\n",
      "[1] \"feature:  33 mean error:  0.1979 sd error:  0.0202\"\n",
      "[1] \"feature:  25 mean error:  0.1913 sd error:  0.0203\"\n",
      "[1] \"feature:  32 mean error:  0.1822 sd error:  0.0254\"\n",
      "[1] \"feature:  19 mean error:  0.176 sd error:  0.0211\"\n",
      "[1] \"feature:  20 mean error:  0.175 sd error:  0.021\"\n",
      "[1] \"feature:  29 mean error:  0.1741 sd error:  0.0224\"\n",
      "[1] \"feature:  21 mean error:  0.174 sd error:  0.0229\"\n",
      "[1] \"feature:  6 mean error:  0.172 sd error:  0.0218\"\n",
      "[1] \"feature:  10 mean error:  0.172 sd error:  0.0216\"\n",
      "[1] \"feature:  23 mean error:  0.1719 sd error:  0.0216\"\n",
      "[1] \"feature:  5 mean error:  0.172 sd error:  0.0215\"\n",
      "[1] \"feature:  59 mean error:  0.1714 sd error:  0.0218\"\n",
      "[1] \"feature:  57 mean error:  0.1715 sd error:  0.0218\"\n",
      "[1] \"feature:  27 mean error:  0.171 sd error:  0.0213\"\n",
      "[1] \"feature:  52 mean error:  0.171 sd error:  0.0215\"\n",
      "[1] \"feature:  9 mean error:  0.1711 sd error:  0.0216\"\n",
      "[1] \"feature:  55 mean error:  0.1696 sd error:  0.0205\"\n",
      "[1] \"feature:  56 mean error:  0.1697 sd error:  0.0205\"\n",
      "[1] \"feature:  3 mean error:  0.1699 sd error:  0.0206\"\n",
      "[1] \"feature:  8 mean error:  0.1701 sd error:  0.0206\"\n",
      "[1] \"feature:  17 mean error:  0.1701 sd error:  0.0205\"\n",
      "[1] \"feature:  38 mean error:  0.1698 sd error:  0.0214\"\n",
      "[1] \"feature:  51 mean error:  0.1683 sd error:  0.022\"\n",
      "[1] \"feature:  37 mean error:  0.1683 sd error:  0.0218\"\n",
      "[1] \"feature:  45 mean error:  0.1684 sd error:  0.0218\"\n",
      "[1] \"feature:  7 mean error:  0.1684 sd error:  0.0219\"\n",
      "[1] \"feature:  34 mean error:  0.1686 sd error:  0.0219\"\n",
      "[1] \"feature:  46 mean error:  0.1689 sd error:  0.022\"\n",
      "[1] \"feature:  63 mean error:  0.1691 sd error:  0.0223\"\n",
      "[1] \"feature:  4 mean error:  0.1694 sd error:  0.0225\"\n",
      "[1] \"feature:  14 mean error:  0.1697 sd error:  0.0225\"\n",
      "[1] \"feature:  50 mean error:  0.1702 sd error:  0.0226\"\n",
      "[1] \"feature:  62 mean error:  0.1707 sd error:  0.0234\"\n",
      "[1] \"feature:  60 mean error:  0.1712 sd error:  0.024\"\n",
      "[1] \"feature:  47 mean error:  0.1713 sd error:  0.024\"\n",
      "[1] \"feature:  1 mean error:  0.1816 sd error:  0.027\"\n",
      "[1] \"feature:  16 mean error:  0.1737 sd error:  0.0222\"\n",
      "[1] \"feature:  39 mean error:  0.1686 sd error:  0.0247\"\n",
      "[1] \"feature:  18 mean error:  0.1686 sd error:  0.0247\"\n",
      "[1] \"feature:  36 mean error:  0.1686 sd error:  0.0247\"\n",
      "[1] \"feature:  12 mean error:  0.1687 sd error:  0.0251\"\n",
      "[1] \"feature:  24 mean error:  0.1689 sd error:  0.0251\"\n",
      "[1] \"feature:  30 mean error:  0.166 sd error:  0.0261\"\n",
      "[1] \"feature:  22 mean error:  0.166 sd error:  0.0261\"\n",
      "[1] \"feature:  11 mean error:  0.1665 sd error:  0.0264\"\n",
      "[1] \"feature:  2 mean error:  0.1771 sd error:  0.0327\"\n",
      "[1] \"feature:  48 mean error:  0.1708 sd error:  0.0278\"\n",
      "[1] \"feature:  35 mean error:  0.1671 sd error:  0.0227\"\n",
      "[1] \"feature:  26 mean error:  0.1655 sd error:  0.0214\"\n",
      "[1] \"feature:  41 mean error:  0.165 sd error:  0.0193\"\n",
      "[1] \"feature:  49 mean error:  0.1646 sd error:  0.0191\"\n",
      "[1] \"feature:  44 mean error:  0.1645 sd error:  0.0209\"\n",
      "[1] \"feature:  54 mean error:  0.164 sd error:  0.023\"\n",
      "[1] \"feature:  28 mean error:  0.164 sd error:  0.023\"\n",
      "[1] \"feature:  42 mean error:  0.164 sd error:  0.023\"\n",
      "[1] \"feature:  53 mean error:  0.1652 sd error:  0.0248\"\n",
      "[1] \"feature:  13 mean error:  0.167 sd error:  0.0295\"\n",
      "[1] \"feature:  15 mean error:  0.167 sd error:  0.0295\"\n",
      "[1] \"feature:  61 mean error:  0.1694 sd error:  0.0299\"\n"
     ]
    }
   ],
   "source": [
    "n <- ncol(dataset)-1\n",
    "nfold <- 10 # number of cross validation\n",
    "cv_size <- nrow(dataset)/nfold # size of one part of the test set used for each validation \n",
    "\n",
    "# selected features\n",
    "selected <- NULL\n",
    "\n",
    "# each features are added to the selected set, one at a time\n",
    "for (i in 1:n) {\n",
    "\n",
    "    # remaining features\n",
    "    candidates <- setdiff(1:n, selected)\n",
    "    # cross validation errors\n",
    "    errors <- matrix(0, nrow=length(candidates), ncol=nfold)\n",
    "    \n",
    "    # test of all the remaining features\n",
    "    for (j in 1:length(candidates)) {\n",
    "        \n",
    "        # set of features tested\n",
    "        features_tested <- c(selected, candidates[j])\n",
    "        \n",
    "        # cross validation\n",
    "        for (k in 1:nfold) {\n",
    "            \n",
    "            # test set\n",
    "            test_set_ind <- ((k-1)*cv_size+1):(k*cv_size)\n",
    "            X_test_set <- dataset[test_set_ind, features_tested, drop=F]\n",
    "            Y_test_set <- dataset[test_set_ind, (n+1)]\n",
    "            \n",
    "            # training set\n",
    "            train_set_ind <- setdiff(1:nrow(dataset), test_set_ind)\n",
    "            X_train_set <- dataset[train_set_ind, features_tested]\n",
    "            Y_train_set <- dataset[train_set_ind, (n+1)]\n",
    "\n",
    "            DS <- as.data.frame(cbind(X_train_set, Y_train_set))\n",
    "            model <- lm(Y_train_set ~ ., DS)\n",
    "            \n",
    "            Y_hat <- predict(model, X_test_set)\n",
    "            \n",
    "            error <- sqrt( mean( (log(Y_hat) - log(Y_test_set))^2, na.rm=T ))\n",
    "            errors[j, k] <- error\n",
    "            \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # select the best feature\n",
    "    error_mean <- apply(errors, 1, mean)\n",
    "    error_sd <- apply(errors, 1, sd)\n",
    "    \n",
    "    best_ind <- which.min(error_mean)\n",
    "    selected <- c(selected, candidates[best_ind])\n",
    "\n",
    "    print(paste(\"feature: \", candidates[best_ind], \"mean error: \", round(error_mean[best_ind], digits=4), \"sd error: \", round(error_sd[best_ind], digits=4)))\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the lowest error is reached with almost all the features. However, the goal was to reduce the number of feature. We can observe a kind of local minimum on the mean error with 30 features. This observation can be used to select only 21 features: 58, 31, 43, 40, 33, 25, 32, 19, 20, 29, 21, 6, 10, 23, 5, 59, 57, 27, 52, 9, 55, 56, 3, 8, 17, 38, 51, 37, 45 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep only the selected features\n",
    "selected_features <- c(58, 31, 43, 40, 33, 25, 32, 19, 20, 29, 21, 6, 10, 23, 5, 59, 57, 27, 52, 9, 55, 56, 3, 8, 17, 38, 51, 37, 45, 7, ncol(dataset))\n",
    "\n",
    "dataset <- dataset[, selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model selection procedure is based on the cross validation technique. To assess that the learning algorithm is able to generalize over all the dataset and does not overfit, the test set should be different than the training set. The cross validation procedure consists in dividing the dataset into n parts of equal size. For each part of the dataset, the learning is done on the overall dataset excluding this part and the training is done on the remaining part. At the end of the procedure, the algorithm returns the mean and the standard deviation of the errors over all the parts.\n",
    "\n",
    "pseudocode:\n",
    "\n",
    "```\n",
    "errors <- list_of_size_n\n",
    "part_size <- nline_dateset / n\n",
    "\n",
    "for i from 1 to n do\n",
    "\n",
    "    test_set <- observations from ((i-1)*part_size+1) to t*part_size\n",
    "    training_set <- remaining observations\n",
    "    \n",
    "    build_model on training_set\n",
    "    errors[i] <- error of model on test_set\n",
    "\n",
    "end_for\n",
    "\n",
    "print mean(errors)\n",
    "print standard_deviation(errors)\n",
    "\n",
    "```\n",
    "\n",
    "Three different models are compared in this procedure. The models considered are linear models (lm), support vector machine (svm) and neural networks (nnet). The neural network needs the dataset to be scaled in order to have good performances. The scaling step consists in substracting the mean of each column of the dataset and dividing the result by the standard deviation. After this step, the mean of each column is equal to 0 and the standard deviation is equal to 1.\n",
    "\n",
    "Moreover, the output of the network is set to the linear function. The prices vector is also scaled. Thanks to this step, the weights of the network are smaller and the accuracy is improved. However, to obtain the final prices after the prediction step, the result of the neural network has to be rescaled (it is multiplied by the standard deviation of the prices of the training set and its mean is added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"lm cross validation error: mean= 0.1684 , sd= 0.0219\"\n",
      "[1] \"svm cross validation error: mean= 0.1913 , sd= 0.086\"\n",
      "[1] \"nnet cross validation error: mean= 0.1781 , sd= 0.0267\"\n"
     ]
    }
   ],
   "source": [
    "library(e1071)\n",
    "library(nnet)\n",
    "\n",
    "n <- ncol(dataset)-1\n",
    "nfold <- 10 # number of cross validation\n",
    "cv_size <- floor(nrow(dataset)/nfold) # size of one part of the test set used for each validation \n",
    "\n",
    "errors_lm <- numeric(nfold)\n",
    "errors_nnet <- numeric(nfold)\n",
    "errors_svm <- numeric(nfold)\n",
    "\n",
    "# the data set is scaled for the neural networks\n",
    "dataset <- cbind(as.data.frame(scale(dataset[,-(n+1)])), dataset[, (n+1)])\n",
    "\n",
    "for (i in 1:nfold) {\n",
    "    \n",
    "    # test set\n",
    "    test_set_ind <- ((i-1)*cv_size+1):(i*cv_size)\n",
    "    X_test_set <- dataset[test_set_ind, -(n+1)]\n",
    "    Y_test_set <- dataset[test_set_ind, (n+1)]\n",
    "    \n",
    "    # training set\n",
    "    train_set_ind <- setdiff(1:nrow(dataset), test_set_ind)\n",
    "    X_train_set <- dataset[train_set_ind, -(n+1)]\n",
    "    Y_train_set <- dataset[train_set_ind, (n+1)]\n",
    "\n",
    "    DS <- as.data.frame(cbind(X_train_set, Y_train_set))\n",
    "    model_svm <- svm(Y_train_set ~ ., DS)\n",
    "    model_lm <- lm(Y_train_set ~ ., DS)\n",
    "    \n",
    "    # othe utput is scaled for the neural network\n",
    "    Y_train_set_mu <- mean(Y_train_set)\n",
    "    Y_train_set_sigma <- sd(Y_train_set)\n",
    "    Y_train_set <- unlist(scale(Y_train_set))\n",
    "    DS <- as.data.frame(cbind(X_train_set, Y_train_set))\n",
    "    model_nnet <- nnet(Y_train_set ~ ., DS, size=5, linout=T, trace=F)\n",
    "    \n",
    "    # prediction with lm\n",
    "    Y_hat <- predict(model_lm, X_test_set)\n",
    "    errors_lm[i] <- sqrt( mean( (log(Y_hat) - log(Y_test_set))^2, na.rm=T ))\n",
    "     \n",
    "    # prediction with svm\n",
    "    Y_hat <- predict(model_svm, X_test_set)\n",
    "    errors_svm[i] <- sqrt( mean( (log(Y_hat) - log(Y_test_set))^2, na.rm=T ))\n",
    "    \n",
    "    # prediction with nnet\n",
    "    Y_hat <- predict(model_nnet, X_test_set)\n",
    "    Y_hat <- Y_hat*Y_train_set_sigma+Y_train_set_mu\n",
    "    errors_nnet[i] <- sqrt( mean( (log(Y_hat) - log(Y_test_set))^2, na.rm=T ))\n",
    "}\n",
    "\n",
    "print(paste(\"lm cross validation error: mean=\", round(mean(errors_lm), digits=4), \", sd=\", round(sd(errors_lm), digits=4)))\n",
    "print(paste(\"svm cross validation error: mean=\", round(mean(errors_svm), digits=4), \", sd=\", round(sd(errors_svm), digits=4)))\n",
    "print(paste(\"nnet cross validation error: mean=\", round(mean(errors_nnet), digits=4), \", sd=\", round(sd(errors_nnet), digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear models and the neural networks give the best results in terms of mean and standard deviation of the error. These models can be used for the ensemble technique. However, the fact that the linear model gives the best result could mean that the assumption made during the feature selection procedure was too strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, multiple models are combined to give a prediction. To build the different models, the bootstrap principle is used: the dataset is resampled with replacements for each model. Moreover, each model is build with a random subset of the features (70% of the features are selected). At the end, the algorithm returns the mean of all the predictions. In this work, neural networks and linear models are combined.\n",
    "\n",
    "pseudocode:\n",
    "\n",
    "```\n",
    "vector_prediction <- vector of size nmodel\n",
    "\n",
    "for i from 1 to nmodel do\n",
    "    # features selected\n",
    "    feature_selected <- choose randomly 0.7*nfeatures\n",
    "    trainig_set <- resample_dataset with chosen_features\n",
    "    \n",
    "    build_model\n",
    "    vector_prediction[i] <- make_prediction(model)\n",
    "    \n",
    "end_for\n",
    "\n",
    "return mean(vector_prediction)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"mean error:  0.2167  ; sd error:  0.0168\"\n"
     ]
    }
   ],
   "source": [
    "N <- nrow(dataset)\n",
    "n <- ncol(dataset)-1\n",
    "\n",
    "nfold <- 10\n",
    "\n",
    "size_cv <- floor(N/nfold)\n",
    "\n",
    "error <- numeric(nfold)\n",
    "\n",
    "nmodel <- 10 # number of models\n",
    "\n",
    "for(i in 1:nfold) {\n",
    "\n",
    "    test_set_ind <- ((i-1)*size_cv+1):(i*size_cv)\n",
    "    X_test_set <- dataset[test_set_ind,-(n+1)]\n",
    "    Y_test_set <- dataset[test_set_ind, (n+1)]\n",
    "\n",
    "    train_set_ind <- setdiff(1:N, test_set_ind)\n",
    "    Y_hat <- matrix(0, nrow=nrow(X_test_set), ncol=nmodel*2)\n",
    "    \n",
    "    # linear model\n",
    "    for(r in 1:nmodel) {\n",
    "\n",
    "        # create a random subset of features\n",
    "        sub_features_ind <- sample(1:n, size=(0.7*n), replace=FALSE)\n",
    "\n",
    "        # observations used to train this model\n",
    "        resample_ind <- sample(train_set_ind, rep=T)\n",
    "\n",
    "        X_train_set <- dataset[resample_ind, sub_features_ind]\n",
    "        Y_train_set <- dataset[resample_ind, (n+1)]\n",
    "\n",
    "        DS <- cbind(X_train_set, Y_train_set)\n",
    "\n",
    "        # build the model\n",
    "        model <- lm(Y_train_set ~ ., DS)\n",
    "\n",
    "        Y_hat[,r] <- predict(model, X_test_set[, sub_features_ind])\n",
    "    }\n",
    "    \n",
    "    # neural network\n",
    "    for(r in 1:nmodel) {\n",
    "\n",
    "        # create a random subset of features\n",
    "        sub_features_ind <- sample(1:n, size=(0.7*n), replace=FALSE)\n",
    "\n",
    "        # observations used to train this model\n",
    "        resample_ind <- sample(train_set_ind, rep=T)\n",
    "\n",
    "        X_train_set <- dataset[resample_ind, sub_features_ind]\n",
    "        Y_train_set <- dataset[resample_ind, (n+1)]\n",
    "\n",
    "        Y_train_mu <- mean(Y_train_set)\n",
    "        Y_train_sigma <- sd(Y_train_set)\n",
    "        Y_train_set <- unlist(scale(Y_train_set))\n",
    "\n",
    "        DS <- cbind(X_train_set, Y_train_set)\n",
    "\n",
    "        # build the model\n",
    "        model <- nnet(Y_train_set ~ ., DS, size=5, linout=T, trace=F)\n",
    "\n",
    "        Y_hat[,r+nmodel] <- predict(model, X_test_set[, sub_features_ind])\n",
    "        Y_hat[,r+nmodel] <- (Y_hat[,r+nmodel]*Y_train_sigma)+Y_train_mu\n",
    "    }\n",
    "\n",
    "    # build the final prediction (average of all the predictions)\n",
    "    Y_hat_mean <- apply(Y_hat, 1, mean)\n",
    "    error[i] <- sqrt( mean( (log(Y_hat_mean)-log(Y_test_set) )^2, na.rm=T))\n",
    "}\n",
    "\n",
    "print(paste(\"mean error: \",round(mean(error),digits=4), \" ; sd error: \",round(sd(error),digits=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the cross validation error is bigger than the previous errors of both models. The ensemble technique seems not to be relevant. However, it could be interesting to try multiple values for the number of features selected and the number of models created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The techniques presented above can be used to obtain the predictions of the test set. An additional difficulty appears during this step: some of the factors are not present in the test set. As a result, after the one hot encoding step, the test set and the training set do not have the same number of variables. To make the predictions, it is necessary to remove the variables that are not present in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(nnet)\n",
    "\n",
    "# load the test set\n",
    "test_set <- read.csv(\"../data/test.csv\")\n",
    "test_set[,2] <- factor(test_set[,2])\n",
    "test_set <- test_set[,-1]\n",
    "test_set_2 <- test_set[, -factor_variables]\n",
    "for (i in 1:(ncol(test_set_2)-1) ) {\n",
    "    val <- mean(test_set_2[,i], na.rm=T)\n",
    "    test_set_2[is.na(test_set_2[,i]),i] <- val\n",
    "}\n",
    "data_factor_onehot <- dummy.data.frame(test_set[,factor_sel], sep=\"_\")\n",
    "test_set <- cbind(data_factor_onehot, test_set_2)\n",
    "test_set <- test_set[, selected_features]\n",
    "test_set <- as.data.frame(scale(test_set))\n",
    "\n",
    "N <- nrow(dataset)\n",
    "n <- ncol(dataset)-1\n",
    "\n",
    "# select features in common in the test and the training set\n",
    "features <- Reduce(intersect, list(v1 = colnames(dataset), v2=colnames(test_set)))\n",
    "test_set <- test_set[, features]\n",
    "dataset <- cbind(dataset[, features], dataset[, (n+1)])\n",
    "\n",
    "n <- ncol(dataset)-1\n",
    "\n",
    "nmodel <- 10 # number of models\n",
    "\n",
    "Y_hat <- matrix(0, nrow=nrow(test_set), ncol=nmodel*2)\n",
    "    \n",
    "# linear model\n",
    "for(r in 1:nmodel) {\n",
    "\n",
    "    # create a random subset of features\n",
    "    sub_features_ind <- sample(1:n, size=(0.7*n), replace=FALSE)\n",
    "\n",
    "    # observations used to train this model\n",
    "    resample_ind <- sample(1:N, rep=T)\n",
    "\n",
    "    X_train_set <- dataset[resample_ind, sub_features_ind]\n",
    "    Y_train_set <- dataset[resample_ind, (n+1)]\n",
    "\n",
    "    DS <- cbind(X_train_set, Y_train_set)\n",
    "\n",
    "    # build the model\n",
    "    model <- lm(Y_train_set ~ ., DS)\n",
    "\n",
    "    Y_hat[,r] <- predict(model, test_set[, sub_features_ind])\n",
    "}\n",
    "    \n",
    "# neural network\n",
    "for(r in 1:nmodel) {\n",
    "\n",
    "    # create a random subset of features\n",
    "    sub_features_ind <- sample(1:n, size=(0.7*n), replace=FALSE)\n",
    "\n",
    "    # observations used to train this model\n",
    "    resample_ind <- sample(1:N, rep=T)\n",
    "\n",
    "    X_train_set <- dataset[resample_ind, sub_features_ind]\n",
    "    Y_train_set <- dataset[resample_ind, (n+1)]\n",
    "\n",
    "    Y_train_mu <- mean(Y_train_set)\n",
    "    Y_train_sigma <- sd(Y_train_set)\n",
    "    Y_train_set <- unlist(scale(Y_train_set))\n",
    "\n",
    "    DS <- cbind(X_train_set, Y_train_set)\n",
    "\n",
    "    # build the model\n",
    "    model <- nnet(Y_train_set ~ ., DS, size=5, linout=T, trace=F)\n",
    "\n",
    "    Y_hat[,r+nmodel] <- predict(model, test_set[, sub_features_ind])\n",
    "    Y_hat[,r+nmodel] <- (Y_hat[,r+nmodel]*Y_train_sigma)+Y_train_mu\n",
    "}\n",
    "\n",
    "# build the final prediction (average of all the predictions)\n",
    "Y_hat_mean <- abs(apply(Y_hat, 1, mean))\n",
    "\n",
    "res <- cbind(read.csv(\"../data/test.csv\")[,1], Y_hat_mean)\n",
    "colnames(res) <- c(\"Id\",\"SalePrice\")\n",
    "write.csv(res, \"res.csv\", quote=F, row.names=F)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the techniques used in this project are not optimal. It gives a score equal to 0.18615.\n",
    "\n",
    "The big number of features is a main difficulty. Indeed, selecting relevant categorical variables without additional knowledge is difficult. An alternative has been tried without the selection of categorical variables. Best results were obtained with a wrapper method using a support vector machine model (final score equal to 0.15).\n",
    "To manage the categorical variables, an unsupervised method for the feature selection could be better. Indeed, the wrapper method relies on a given model, which was the best in the cross validation procedure.\n",
    "\n",
    "Another way to reduce the number of features would be to ask an expert what the more relevant features are according to him. This step can be used as a preprocessing, before using a method like the wrapper one. We could also analyse the number of factors of the categorical variables and ignore some variables if they have very few factors.\n",
    "\n",
    "In addition, the ensemble of models procedure and the selection of models procedure can be improved by tuning the parameters (for example, the number of neurones in the hidden layer, the number of model used). However, this step requires a lot of computational resources. Another way to improve the ensemble of models technique is to give a weight to each model and compute a weighted sum for the final prediction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
